{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load/import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T18:22:03.227240Z",
     "iopub.status.busy": "2020-11-30T18:22:03.227240Z",
     "iopub.status.idle": "2020-11-30T18:22:07.014099Z",
     "shell.execute_reply": "2020-11-30T18:22:07.013096Z",
     "shell.execute_reply.started": "2020-11-30T18:22:03.227240Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import scipy\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from os import mkdir\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import modules to run custom FW-RNN cell\n",
    "from tensorflow.python.keras.layers.recurrent import (\n",
    "    _generate_zero_filled_state_for_cell,\n",
    "    _generate_zero_filled_state,\n",
    "    activations,\n",
    "    initializers,\n",
    "    regularizers,\n",
    "    nest\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Limit GPU memory usage\n",
    "for device in tf.config.experimental.list_physical_devices(\"GPU\"):\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T18:22:07.015097Z",
     "iopub.status.busy": "2020-11-30T18:22:07.015097Z",
     "iopub.status.idle": "2020-11-30T18:22:16.866150Z",
     "shell.execute_reply": "2020-11-30T18:22:16.866150Z",
     "shell.execute_reply.started": "2020-11-30T18:22:07.015097Z"
    }
   },
   "outputs": [],
   "source": [
    "# load Aff-Wild2 Features\n",
    "train_features_AW2 = scipy.sparse.load_npz(\"data/features/train_features_RGB_AW2.npz\") #CSR Matrix\n",
    "val_features_AW2 = scipy.sparse.load_npz(\"data/features/val_features_RGB_AW2.npz\") #CSR Matrix\n",
    "train_labels_AW2 = np.load(\"data/labels/train_labels_RGB_AW2.npy\") #Numpy array\n",
    "val_labels_AW2 = np.load(\"data/labels/val_labels_RGB_AW2.npy\") #Numpy array\n",
    "\n",
    "# test_features_AW2 = scipy.sparse.load_npz(\"data/features/test_features_RGB_AW2.npz\") #CSR Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T19:16:23.488126Z",
     "iopub.status.busy": "2020-11-30T19:16:23.487127Z",
     "iopub.status.idle": "2020-11-30T19:16:23.946140Z",
     "shell.execute_reply": "2020-11-30T19:16:23.946140Z",
     "shell.execute_reply.started": "2020-11-30T19:16:23.488126Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load AFEW7.0 features\n",
    "test_features_AF7 = scipy.sparse.load_npz(\"data/features/features_RGB_AF7.npz\") #CSR Matrix\n",
    "test_labels_AF7 = np.load(\"data/labels/labels_RGB_AF7.npy\") #Numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T18:22:16.867152Z",
     "iopub.status.busy": "2020-11-30T18:22:16.867152Z",
     "iopub.status.idle": "2020-11-30T18:22:16.883154Z",
     "shell.execute_reply": "2020-11-30T18:22:16.882152Z",
     "shell.execute_reply.started": "2020-11-30T18:22:16.867152Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_history(data_list, label_list, title, ylabel):\n",
    "\n",
    "    epochs = range(1, len(data_list[0]) + 1)\n",
    "\n",
    "    for data, label in zip(data_list, label_list):\n",
    "        plt.plot(epochs, data, label=label)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "# Function which reshapes the features to [sequences, sequence_length, features], \n",
    "# and labels to [sequences, sequence_length, labels]\n",
    "def labels_reshaper(labels, sequence_length):\n",
    "    # Find the amount of possible sequences with given sequence_length. Some data will be discarded this way.\n",
    "    amount = labels.shape[0] // sequence_length\n",
    "    \n",
    "    # Reshapes the labels\n",
    "    seq_labels = np.reshape(labels[:(amount * sequence_length)], (amount, sequence_length, labels.shape[1]))\n",
    "\n",
    "    return seq_labels\n",
    "\n",
    "# Function which reshapes the features to [sequences, sequence_length, features],\n",
    "def features_reshaper(features, sequence_length):\n",
    "    amount = features.shape[0] // sequence_length\n",
    "    arr_features = features[:(amount * sequence_length)].toarray()\n",
    "    \n",
    "    # Reshapes the features\n",
    "    seq_features = np.reshape(arr_features, (amount, sequence_length, arr_features.shape[1]))\n",
    "    \n",
    "    return seq_features\n",
    "\n",
    "\n",
    "def arr_replacevalue(array, d_class_weights, start=0):\n",
    "    labels = d_class_weights.keys()\n",
    "    labels = list(labels)\n",
    "    if start > 6:\n",
    "        return array\n",
    "    array = np.where(array ==  labels[start], d_class_weights.get(labels[start]), array)\n",
    "\n",
    "    return arr_replacevalue(array, d_class_weights, start + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T18:22:16.884153Z",
     "iopub.status.busy": "2020-11-30T18:22:16.883154Z",
     "iopub.status.idle": "2020-11-30T18:22:22.955152Z",
     "shell.execute_reply": "2020-11-30T18:22:22.954150Z",
     "shell.execute_reply.started": "2020-11-30T18:22:16.884153Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reshape data to specified sequence length\n",
    "length = 60\n",
    "\n",
    "X_train = features_reshaper(train_features_AW2, length) \n",
    "del train_features_AW2 # wipe out of memory to free up space\n",
    "\n",
    "X_val = features_reshaper(val_features_AW2, length)\n",
    "del val_features_AW2 # wipe out of memory to free up space\n",
    "\n",
    "y_train = labels_reshaper(train_labels_AW2, length)\n",
    "del train_labels_AW2  # wipe out of memory to free up space\n",
    "\n",
    "y_val = labels_reshaper(val_labels_AW2, length)\n",
    "del val_labels_AW2  # wipe out of memory to free up space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T19:16:28.433073Z",
     "iopub.status.busy": "2020-11-30T19:16:28.433073Z",
     "iopub.status.idle": "2020-11-30T19:16:28.713072Z",
     "shell.execute_reply": "2020-11-30T19:16:28.713072Z",
     "shell.execute_reply.started": "2020-11-30T19:16:28.433073Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test_AF7 = features_reshaper(test_features_AF7, length) \n",
    "del test_features_AF7 # wipe out of memory to free up space\n",
    "\n",
    "y_test_AF7 = labels_reshaper(test_labels_AF7, length)\n",
    "del test_labels_AF7 # wipe out of memory to free up space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T18:22:22.955152Z",
     "iopub.status.busy": "2020-11-30T18:22:22.955152Z",
     "iopub.status.idle": "2020-11-30T18:22:28.162152Z",
     "shell.execute_reply": "2020-11-30T18:22:28.161153Z",
     "shell.execute_reply.started": "2020-11-30T18:22:22.955152Z"
    }
   },
   "outputs": [],
   "source": [
    "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size = 0.5, random_state = 1234, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T18:22:28.165153Z",
     "iopub.status.busy": "2020-11-30T18:22:28.164153Z",
     "iopub.status.idle": "2020-11-30T18:22:28.510152Z",
     "shell.execute_reply": "2020-11-30T18:22:28.509154Z",
     "shell.execute_reply.started": "2020-11-30T18:22:28.165153Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ion\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass classes=[0 1 2 3 4 5 6], y=[0 0 0 ... 6 6 6] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "def comp_sampleweights(labels):\n",
    "    # Convert one-hot encoded labels back to label integers\n",
    "    train_label_ints = np.argmax(labels, axis=2)\n",
    "\n",
    "    # Compute class weights with sklearn\n",
    "    class_weights = class_weight.compute_class_weight(\n",
    "        \"balanced\", np.unique(train_label_ints), train_label_ints.flatten()\n",
    "    )\n",
    "    d_class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "    # Pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample\n",
    "    return arr_replacevalue(train_label_ints, d_class_weights)\n",
    "    \n",
    "train_samples_weights = comp_sampleweights(y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T15:27:37.993463Z",
     "iopub.status.busy": "2020-11-11T15:27:37.993463Z",
     "iopub.status.idle": "2020-11-11T15:27:38.000462Z",
     "shell.execute_reply": "2020-11-11T15:27:38.000462Z",
     "shell.execute_reply.started": "2020-11-11T15:27:37.993463Z"
    }
   },
   "source": [
    "# Build FW-RNN model\n",
    "-  Build custom FW_RNN cell and wrap it in RNN layer (https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN), like this: RNN(FW_RNN)\n",
    "    -  \"The cell abstraction, together with the generic keras.layers.RNN class, make it very easy to implement custom RNN architectures for your research.\"\n",
    "\n",
    "Created by using this guide: https://www.tensorflow.org/guide/keras/custom_layers_and_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T19:21:55.561427Z",
     "iopub.status.busy": "2020-11-30T19:21:55.561427Z",
     "iopub.status.idle": "2020-11-30T19:21:55.585425Z",
     "shell.execute_reply": "2020-11-30T19:21:55.585425Z",
     "shell.execute_reply.started": "2020-11-30T19:21:55.561427Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build model with sequential api\n",
    "def build_FWRNN(batch, units, activation_function):\n",
    "    class FW_RNNCell(layers.Layer):\n",
    "        def __init__(\n",
    "            self,\n",
    "            units,\n",
    "            use_bias,\n",
    "            batch_size,\n",
    "            decay_rate,\n",
    "            learning_rate,\n",
    "            activation,\n",
    "            step,\n",
    "            LN=layers.LayerNormalization(),\n",
    "            **kwargs\n",
    "        ):\n",
    "            super(FW_RNNCell, self).__init__(**kwargs)\n",
    "            self.units = units\n",
    "            self.step = step\n",
    "            self.use_bias = use_bias\n",
    "            self.activation = activations.get(activation)\n",
    "            self.l = decay_rate\n",
    "            self.e = learning_rate\n",
    "            self.LN = LN\n",
    "            self.batch = batch_size\n",
    "            self.state_size = self.units\n",
    "\n",
    "            # Initializer & regularizer for the slow input-to-hidden weights matrix\n",
    "            self.C_initializer = initializers.get(\"glorot_uniform\")\n",
    "\n",
    "            # Initializer & regularizer for the slow hidden weights matrix\n",
    "            self.W_h_initializer = initializers.get(\"identity\")\n",
    "\n",
    "            # Initializer & regularizer for the fast weights matrix\n",
    "            self.A_initializer = initializers.get(\"zeros\")\n",
    "\n",
    "            # Initializer for the bias vector.\n",
    "            self.b_x_initializer = initializers.get(\"zeros\")\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            # Build is only called at the start, to initialize all the weights and biases\n",
    "\n",
    "            # C = Slow input-to-hidden weights [shape (4608, 64)]\n",
    "            self.C = self.add_weight(\n",
    "                shape=(input_shape[-1], self.units),\n",
    "                name=\"inputweights\",\n",
    "                initializer=self.C_initializer,\n",
    "            )\n",
    "\n",
    "            # W_h The previous hidden state via the slow transition weights [shape (units, units)]\n",
    "            # they suggest to multiply it with 0.05, so gain = 0.05\n",
    "            self.W_h = self.add_weight(\n",
    "                shape=(self.units, self.units),\n",
    "                name=\"hiddenweights\",\n",
    "                initializer=self.W_h_initializer,\n",
    "            )\n",
    "            self.W_h = tf.scalar_mul(0.05, self.W_h)\n",
    "\n",
    "            # A (fast weights) [shape (batch_size, units, units)]\n",
    "            self.A = self.add_weight(\n",
    "                shape=(self.batch, self.units, self.units),\n",
    "                name=\"fastweights\",\n",
    "                initializer=self.A_initializer,\n",
    "            )\n",
    "\n",
    "            if self.use_bias:\n",
    "                self.bias = self.add_weight(\n",
    "                    shape=(self.units,), name=\"bias\", initializer=self.b_x_initializer,\n",
    "                )\n",
    "            else:\n",
    "                self.bias = None\n",
    "            self.built = True\n",
    "\n",
    "        def call(self, inputs, states, training=None):\n",
    "            prev_output = states[0] if nest.is_sequence(states) else states\n",
    "\n",
    "            # Next hidden state h(t+1) is computed in two steps:\n",
    "            # Step 1 calculate preliminary vector: h_0(t+1) = f(W_h ⋅ h(t) + C ⋅ x(t))\n",
    "            h = K.dot(prev_output, self.W_h) + K.dot(inputs, self.C)\n",
    "            if self.bias is not None:\n",
    "                h = h + self.bias\n",
    "            if self.activation is not None:\n",
    "                h = self.activation(h)\n",
    "\n",
    "            # Reshape h to use with a\n",
    "            h_s = tf.reshape(h, [self.batch, 1, self.units])\n",
    "\n",
    "            # Define preliminary vector in variable\n",
    "            prelim = tf.reshape(K.dot(prev_output, self.W_h), (h_s.shape)) + tf.reshape(\n",
    "                K.dot(inputs, self.C), (h_s.shape)\n",
    "            )\n",
    "\n",
    "            # Fast weights update rule: A(t) = λ*A(t-1) + η*h(t) ⋅ h(t)^T\n",
    "            self.A.assign(\n",
    "                tf.math.add(\n",
    "                    tf.scalar_mul(self.l, self.A),\n",
    "                    tf.scalar_mul(\n",
    "                        self.e, tf.linalg.matmul(tf.transpose(h_s, [0, 2, 1]), h_s)\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Step 2: Initiate inner loop with preliminary vector, which runs for S steps\n",
    "            # to progressively change the hidden state into h(t+1) = h_s(t+1)\n",
    "            # h_s+1(t+1) f([W_h ⋅ h(t) + C ⋅ x(t)]) + A(t)h_s(t+1)\n",
    "            for _ in range(self.step):\n",
    "                h_s = tf.math.add(prelim, tf.linalg.matmul(h_s, self.A))\n",
    "                if self.activation is not None:\n",
    "                    h_s = self.activation(h_s)\n",
    "\n",
    "                # Apply layer normalization on hidden state\n",
    "                h_s = self.LN(h_s)\n",
    "\n",
    "            h = tf.reshape(h_s, [self.batch, self.units])\n",
    "\n",
    "            output = h\n",
    "            new_state = [output] if nest.is_sequence(states) else output\n",
    "            return output, new_state\n",
    "\n",
    "        def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n",
    "            return _generate_zero_filled_state_for_cell(self, inputs, batch_size, dtype)\n",
    "\n",
    "        def get_config(self):\n",
    "            config = {\n",
    "                \"units\": self.units,\n",
    "                \"step\": self.step,\n",
    "                \"batch_size\": self.batch,\n",
    "                \"use_bias\": self.use_bias,\n",
    "                \"activation\": activations.serialize(self.activation),\n",
    "                \"decay_rate\": self.l,\n",
    "                \"learning_rate\": self.e,\n",
    "                \"LN\": self.LN,\n",
    "            }\n",
    "            base_config = super(FW_RNNCell, self).get_config()\n",
    "            return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    # Define model\n",
    "    model = Sequential(name=\"FW-RNN\")\n",
    "    model.add(\n",
    "        tf.keras.Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "    )\n",
    "    model.add(\n",
    "        layers.RNN(\n",
    "            FW_RNNCell(\n",
    "                units=units,\n",
    "                use_bias=True,\n",
    "                activation=activation_function,\n",
    "                step=1,\n",
    "                decay_rate=0.95,\n",
    "                learning_rate=0.5,\n",
    "                batch_size=batch,\n",
    "            ),\n",
    "            return_sequences=True,\n",
    "            name=\"FW-RNN\",\n",
    "        )\n",
    "    )\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    model.add(layers.Dense(7, activation=\"softmax\", name=\"Dense_Output\"))\n",
    "    model.compile(\n",
    "        optimizer = 'rmsprop',\n",
    "        loss=CategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"],\n",
    "        run_eagerly=False,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T19:21:55.940067Z",
     "iopub.status.busy": "2020-11-30T19:21:55.940067Z",
     "iopub.status.idle": "2020-11-30T19:21:55.951066Z",
     "shell.execute_reply": "2020-11-30T19:21:55.951066Z",
     "shell.execute_reply.started": "2020-11-30T19:21:55.940067Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build baseline model (RNN or LSTM) with sequential api\n",
    "def build_base(model_name, units):\n",
    "    model = Sequential(name=model_name)\n",
    "    model.add(\n",
    "        tf.keras.Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "    )\n",
    "    if model_name == \"RNN\":\n",
    "        model.add(layers.SimpleRNN(units, return_sequences=True))\n",
    "        model.add(layers.Dropout(0.4))\n",
    "    else:\n",
    "        model.add(layers.LSTM(units, return_sequences=True))\n",
    "        model.add(layers.Dropout(0.4))\n",
    "    model.add(layers.LayerNormalization())\n",
    "    model.add(layers.Dense(7, activation=\"softmax\", name=\"Dense_Output\"))\n",
    "    model.compile(\n",
    "            optimizer='rmsprop',\n",
    "            loss=CategoricalCrossentropy(),\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T19:21:56.988068Z",
     "iopub.status.busy": "2020-11-30T19:21:56.987070Z",
     "iopub.status.idle": "2020-11-30T19:21:56.995069Z",
     "shell.execute_reply": "2020-11-30T19:21:56.994068Z",
     "shell.execute_reply.started": "2020-11-30T19:21:56.988068Z"
    }
   },
   "outputs": [],
   "source": [
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0.0025, patience=6, verbose=1, mode='auto', restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T19:23:31.363196Z",
     "iopub.status.busy": "2020-11-30T19:23:31.363196Z",
     "iopub.status.idle": "2020-11-30T20:09:00.386494Z",
     "shell.execute_reply": "2020-11-30T20:09:00.386494Z",
     "shell.execute_reply.started": "2020-11-30T19:23:31.363196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"RNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_4 (SimpleRNN)     (None, 60, 5)             23070     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 60, 5)             0         \n",
      "_________________________________________________________________\n",
      "layer_normalization_57 (Laye (None, 60, 5)             10        \n",
      "_________________________________________________________________\n",
      "Dense_Output (Dense)         (None, 60, 7)             42        \n",
      "=================================================================\n",
      "Total params: 23,122\n",
      "Trainable params: 23,122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "480/480 - 26s - loss: 1.8744 - accuracy: 0.4003 - val_loss: 1.9439 - val_accuracy: 0.1839\n",
      "Epoch 2/50\n",
      "480/480 - 21s - loss: 1.6382 - accuracy: 0.4039 - val_loss: 1.8867 - val_accuracy: 0.3037\n",
      "Epoch 3/50\n",
      "480/480 - 21s - loss: 1.5820 - accuracy: 0.4373 - val_loss: 1.8552 - val_accuracy: 0.3112\n",
      "Epoch 4/50\n",
      "480/480 - 22s - loss: 1.5378 - accuracy: 0.4440 - val_loss: 1.7802 - val_accuracy: 0.3920\n",
      "Epoch 5/50\n",
      "480/480 - 23s - loss: 1.5118 - accuracy: 0.4491 - val_loss: 1.8032 - val_accuracy: 0.4277\n",
      "Epoch 6/50\n",
      "480/480 - 22s - loss: 1.5005 - accuracy: 0.4524 - val_loss: 1.9150 - val_accuracy: 0.3447\n",
      "Epoch 7/50\n",
      "480/480 - 22s - loss: 1.4887 - accuracy: 0.4532 - val_loss: 1.8506 - val_accuracy: 0.3819\n",
      "Epoch 8/50\n",
      "480/480 - 23s - loss: 1.4823 - accuracy: 0.4300 - val_loss: 1.8444 - val_accuracy: 0.3777\n",
      "Epoch 9/50\n",
      "480/480 - 23s - loss: 1.4715 - accuracy: 0.4393 - val_loss: 1.8827 - val_accuracy: 0.3792\n",
      "Epoch 10/50\n",
      "480/480 - 21s - loss: 1.4623 - accuracy: 0.4399 - val_loss: 1.8403 - val_accuracy: 0.4108\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Model: \"LSTM\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 60, 5)             92280     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 60, 5)             0         \n",
      "_________________________________________________________________\n",
      "layer_normalization_58 (Laye (None, 60, 5)             10        \n",
      "_________________________________________________________________\n",
      "Dense_Output (Dense)         (None, 60, 7)             42        \n",
      "=================================================================\n",
      "Total params: 92,332\n",
      "Trainable params: 92,332\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "480/480 - 11s - loss: 1.8808 - accuracy: 0.3466 - val_loss: 1.9575 - val_accuracy: 0.3396\n",
      "Epoch 2/50\n",
      "480/480 - 9s - loss: 1.7565 - accuracy: 0.3040 - val_loss: 2.0092 - val_accuracy: 0.1695\n",
      "Epoch 3/50\n",
      "480/480 - 9s - loss: 1.6988 - accuracy: 0.2652 - val_loss: 1.9787 - val_accuracy: 0.3591\n",
      "Epoch 4/50\n",
      "480/480 - 9s - loss: 1.6605 - accuracy: 0.4009 - val_loss: 2.0430 - val_accuracy: 0.3728\n",
      "Epoch 5/50\n",
      "480/480 - 9s - loss: 1.6408 - accuracy: 0.4461 - val_loss: 1.9084 - val_accuracy: 0.4589\n",
      "Epoch 6/50\n",
      "480/480 - 10s - loss: 1.6217 - accuracy: 0.4486 - val_loss: 1.9708 - val_accuracy: 0.4304\n",
      "Epoch 7/50\n",
      "480/480 - 10s - loss: 1.6041 - accuracy: 0.4567 - val_loss: 1.9268 - val_accuracy: 0.4312\n",
      "Epoch 8/50\n",
      "480/480 - 9s - loss: 1.6042 - accuracy: 0.4663 - val_loss: 1.9286 - val_accuracy: 0.4146\n",
      "Epoch 9/50\n",
      "480/480 - 9s - loss: 1.5890 - accuracy: 0.4140 - val_loss: 1.8543 - val_accuracy: 0.3330\n",
      "Epoch 10/50\n",
      "480/480 - 9s - loss: 1.5812 - accuracy: 0.3994 - val_loss: 1.8134 - val_accuracy: 0.3988\n",
      "Epoch 11/50\n",
      "480/480 - 9s - loss: 1.5606 - accuracy: 0.4274 - val_loss: 1.9963 - val_accuracy: 0.3835\n",
      "Epoch 12/50\n",
      "480/480 - 9s - loss: 1.5260 - accuracy: 0.4770 - val_loss: 1.9312 - val_accuracy: 0.4004\n",
      "Epoch 13/50\n",
      "480/480 - 9s - loss: 1.5184 - accuracy: 0.4791 - val_loss: 1.8744 - val_accuracy: 0.4318\n",
      "Epoch 14/50\n",
      "480/480 - 9s - loss: 1.4987 - accuracy: 0.4810 - val_loss: 1.9138 - val_accuracy: 0.3937\n",
      "Epoch 15/50\n",
      "480/480 - 9s - loss: 1.4862 - accuracy: 0.4823 - val_loss: 1.8848 - val_accuracy: 0.3992\n",
      "Epoch 16/50\n",
      "480/480 - 9s - loss: 1.4740 - accuracy: 0.4835 - val_loss: 1.9197 - val_accuracy: 0.3838\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Model: \"FW-RNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "FW-RNN (RNN)                 (32, 60, 5)               23855     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (32, 60, 5)               0         \n",
      "_________________________________________________________________\n",
      "Dense_Output (Dense)         (32, 60, 7)               42        \n",
      "=================================================================\n",
      "Total params: 23,897\n",
      "Trainable params: 23,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "480/480 - 41s - loss: 1.9022 - accuracy: 0.2736 - val_loss: 1.9960 - val_accuracy: 0.1825\n",
      "Epoch 2/50\n",
      "480/480 - 39s - loss: 1.6648 - accuracy: 0.3520 - val_loss: 1.8208 - val_accuracy: 0.3360\n",
      "Epoch 3/50\n",
      "480/480 - 39s - loss: 1.5406 - accuracy: 0.4371 - val_loss: 1.7335 - val_accuracy: 0.2965\n",
      "Epoch 4/50\n",
      "480/480 - 38s - loss: 1.4649 - accuracy: 0.4078 - val_loss: 1.7755 - val_accuracy: 0.3597\n",
      "Epoch 5/50\n",
      "480/480 - 40s - loss: 1.4023 - accuracy: 0.3932 - val_loss: 1.7879 - val_accuracy: 0.2818\n",
      "Epoch 6/50\n",
      "480/480 - 38s - loss: 1.3730 - accuracy: 0.3882 - val_loss: 1.7345 - val_accuracy: 0.4000\n",
      "Epoch 7/50\n",
      "480/480 - 37s - loss: 1.3552 - accuracy: 0.4107 - val_loss: 1.8190 - val_accuracy: 0.2898\n",
      "Epoch 8/50\n",
      "480/480 - 38s - loss: 1.3437 - accuracy: 0.3649 - val_loss: 1.6477 - val_accuracy: 0.4876\n",
      "Epoch 9/50\n",
      "480/480 - 39s - loss: 1.3420 - accuracy: 0.4246 - val_loss: 1.7104 - val_accuracy: 0.3685\n",
      "Epoch 10/50\n",
      "480/480 - 38s - loss: 1.3211 - accuracy: 0.4159 - val_loss: 1.7446 - val_accuracy: 0.3348\n",
      "Epoch 11/50\n",
      "480/480 - 38s - loss: 1.3152 - accuracy: 0.4248 - val_loss: 1.7272 - val_accuracy: 0.3958\n",
      "Epoch 12/50\n",
      "480/480 - 38s - loss: 1.3087 - accuracy: 0.4324 - val_loss: 1.6868 - val_accuracy: 0.4202\n",
      "Epoch 13/50\n",
      "480/480 - 39s - loss: 1.2939 - accuracy: 0.4634 - val_loss: 1.7133 - val_accuracy: 0.3846\n",
      "Epoch 14/50\n",
      "480/480 - 38s - loss: 1.2937 - accuracy: 0.4472 - val_loss: 1.8775 - val_accuracy: 0.3546\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Model: \"RNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_5 (SimpleRNN)     (None, 60, 20)            92580     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 60, 20)            0         \n",
      "_________________________________________________________________\n",
      "layer_normalization_60 (Laye (None, 60, 20)            40        \n",
      "_________________________________________________________________\n",
      "Dense_Output (Dense)         (None, 60, 7)             147       \n",
      "=================================================================\n",
      "Total params: 92,767\n",
      "Trainable params: 92,767\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "480/480 - 21s - loss: 1.6549 - accuracy: 0.4145 - val_loss: 1.6850 - val_accuracy: 0.3653\n",
      "Epoch 2/50\n",
      "480/480 - 20s - loss: 1.2554 - accuracy: 0.5505 - val_loss: 1.7335 - val_accuracy: 0.3928\n",
      "Epoch 3/50\n",
      "480/480 - 20s - loss: 1.1537 - accuracy: 0.5897 - val_loss: 1.8502 - val_accuracy: 0.3576\n",
      "Epoch 4/50\n",
      "480/480 - 22s - loss: 1.0871 - accuracy: 0.6071 - val_loss: 1.8027 - val_accuracy: 0.4060\n",
      "Epoch 5/50\n",
      "480/480 - 24s - loss: 1.0622 - accuracy: 0.6190 - val_loss: 1.8653 - val_accuracy: 0.3799\n",
      "Epoch 6/50\n",
      "480/480 - 22s - loss: 1.0287 - accuracy: 0.6228 - val_loss: 1.8907 - val_accuracy: 0.3985\n",
      "Epoch 7/50\n",
      "480/480 - 22s - loss: 1.0185 - accuracy: 0.6289 - val_loss: 1.7705 - val_accuracy: 0.4188\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Model: \"LSTM\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 60, 20)            370320    \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 60, 20)            0         \n",
      "_________________________________________________________________\n",
      "layer_normalization_61 (Laye (None, 60, 20)            40        \n",
      "_________________________________________________________________\n",
      "Dense_Output (Dense)         (None, 60, 7)             147       \n",
      "=================================================================\n",
      "Total params: 370,507\n",
      "Trainable params: 370,507\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "480/480 - 11s - loss: 1.6388 - accuracy: 0.4210 - val_loss: 1.7633 - val_accuracy: 0.3450\n",
      "Epoch 2/50\n",
      "480/480 - 9s - loss: 1.2528 - accuracy: 0.5308 - val_loss: 1.8196 - val_accuracy: 0.3441\n",
      "Epoch 3/50\n",
      "480/480 - 9s - loss: 1.1311 - accuracy: 0.5862 - val_loss: 1.9079 - val_accuracy: 0.3482\n",
      "Epoch 4/50\n",
      "480/480 - 9s - loss: 1.0686 - accuracy: 0.6084 - val_loss: 1.8021 - val_accuracy: 0.4115\n",
      "Epoch 5/50\n",
      "480/480 - 9s - loss: 1.0081 - accuracy: 0.6265 - val_loss: 1.8623 - val_accuracy: 0.3965\n",
      "Epoch 6/50\n",
      "480/480 - 9s - loss: 0.9648 - accuracy: 0.6471 - val_loss: 1.8753 - val_accuracy: 0.4064\n",
      "Epoch 7/50\n",
      "480/480 - 9s - loss: 0.9383 - accuracy: 0.6594 - val_loss: 1.9174 - val_accuracy: 0.3934\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Model: \"FW-RNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "FW-RNN (RNN)                 (32, 60, 20)              105020    \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (32, 60, 20)              0         \n",
      "_________________________________________________________________\n",
      "Dense_Output (Dense)         (32, 60, 7)               147       \n",
      "=================================================================\n",
      "Total params: 105,167\n",
      "Trainable params: 105,167\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "480/480 - 41s - loss: 1.8063 - accuracy: 0.3202 - val_loss: 1.8643 - val_accuracy: 0.2442\n",
      "Epoch 2/50\n",
      "480/480 - 39s - loss: 1.4414 - accuracy: 0.4289 - val_loss: 1.7562 - val_accuracy: 0.2854\n",
      "Epoch 3/50\n",
      "480/480 - 40s - loss: 1.3347 - accuracy: 0.4548 - val_loss: 1.7992 - val_accuracy: 0.3435\n",
      "Epoch 4/50\n",
      "480/480 - 39s - loss: 1.2544 - accuracy: 0.4992 - val_loss: 1.7491 - val_accuracy: 0.3615\n",
      "Epoch 5/50\n",
      "480/480 - 39s - loss: 1.2037 - accuracy: 0.5312 - val_loss: 1.7599 - val_accuracy: 0.3706\n",
      "Epoch 6/50\n",
      "480/480 - 40s - loss: 1.1751 - accuracy: 0.5380 - val_loss: 1.7825 - val_accuracy: 0.3663\n",
      "Epoch 7/50\n",
      "480/480 - 40s - loss: 1.1432 - accuracy: 0.5450 - val_loss: 1.7970 - val_accuracy: 0.3793\n",
      "Epoch 8/50\n",
      "480/480 - 40s - loss: 1.1162 - accuracy: 0.5579 - val_loss: 1.7669 - val_accuracy: 0.3688\n",
      "Epoch 9/50\n",
      "480/480 - 38s - loss: 1.0962 - accuracy: 0.5627 - val_loss: 1.8584 - val_accuracy: 0.3580\n",
      "Epoch 10/50\n",
      "480/480 - 39s - loss: 1.0798 - accuracy: 0.5768 - val_loss: 1.7756 - val_accuracy: 0.3903\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Model: \"RNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_6 (SimpleRNN)     (None, 60, 50)            232950    \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 60, 50)            0         \n",
      "_________________________________________________________________\n",
      "layer_normalization_63 (Laye (None, 60, 50)            100       \n",
      "_________________________________________________________________\n",
      "Dense_Output (Dense)         (None, 60, 7)             357       \n",
      "=================================================================\n",
      "Total params: 233,407\n",
      "Trainable params: 233,407\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "480/480 - 23s - loss: 1.4890 - accuracy: 0.4722 - val_loss: 1.8076 - val_accuracy: 0.3436\n",
      "Epoch 2/50\n",
      "480/480 - 21s - loss: 1.0926 - accuracy: 0.5946 - val_loss: 1.7830 - val_accuracy: 0.3728\n",
      "Epoch 3/50\n",
      "480/480 - 21s - loss: 0.9875 - accuracy: 0.6371 - val_loss: 1.8268 - val_accuracy: 0.3851\n",
      "Epoch 4/50\n",
      "480/480 - 22s - loss: 0.9384 - accuracy: 0.6537 - val_loss: 1.8055 - val_accuracy: 0.4254\n",
      "Epoch 5/50\n",
      "480/480 - 22s - loss: 0.8917 - accuracy: 0.6641 - val_loss: 1.8954 - val_accuracy: 0.3854\n",
      "Epoch 6/50\n",
      "480/480 - 21s - loss: 0.8534 - accuracy: 0.6754 - val_loss: 1.8149 - val_accuracy: 0.4417\n",
      "Epoch 7/50\n",
      "480/480 - 22s - loss: 0.8391 - accuracy: 0.6839 - val_loss: 1.9255 - val_accuracy: 0.3817\n",
      "Epoch 8/50\n",
      "480/480 - 22s - loss: 0.8221 - accuracy: 0.6883 - val_loss: 1.9208 - val_accuracy: 0.4244\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Model: \"LSTM\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 60, 50)            931800    \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 60, 50)            0         \n",
      "_________________________________________________________________\n",
      "layer_normalization_64 (Laye (None, 60, 50)            100       \n",
      "_________________________________________________________________\n",
      "Dense_Output (Dense)         (None, 60, 7)             357       \n",
      "=================================================================\n",
      "Total params: 932,257\n",
      "Trainable params: 932,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "480/480 - 11s - loss: 1.5059 - accuracy: 0.5021 - val_loss: 1.6868 - val_accuracy: 0.3858\n",
      "Epoch 2/50\n",
      "480/480 - 9s - loss: 1.1047 - accuracy: 0.6052 - val_loss: 1.6171 - val_accuracy: 0.4236\n",
      "Epoch 3/50\n",
      "480/480 - 9s - loss: 0.9545 - accuracy: 0.6634 - val_loss: 1.7393 - val_accuracy: 0.4001\n",
      "Epoch 4/50\n",
      "480/480 - 9s - loss: 0.8708 - accuracy: 0.6872 - val_loss: 1.8036 - val_accuracy: 0.4282\n",
      "Epoch 5/50\n",
      "480/480 - 9s - loss: 0.8226 - accuracy: 0.7058 - val_loss: 1.7581 - val_accuracy: 0.4522\n",
      "Epoch 6/50\n",
      "480/480 - 9s - loss: 0.7819 - accuracy: 0.7200 - val_loss: 1.9098 - val_accuracy: 0.4433\n",
      "Epoch 7/50\n",
      "480/480 - 9s - loss: 0.7502 - accuracy: 0.7305 - val_loss: 1.9839 - val_accuracy: 0.4213\n",
      "Epoch 8/50\n",
      "480/480 - 9s - loss: 0.7073 - accuracy: 0.7421 - val_loss: 1.8552 - val_accuracy: 0.4706\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Model: \"FW-RNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "FW-RNN (RNN)                 (32, 60, 50)              310550    \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (32, 60, 50)              0         \n",
      "_________________________________________________________________\n",
      "Dense_Output (Dense)         (32, 60, 7)               357       \n",
      "=================================================================\n",
      "Total params: 310,907\n",
      "Trainable params: 310,907\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "480/480 - 41s - loss: 1.7822 - accuracy: 0.3347 - val_loss: 1.9097 - val_accuracy: 0.3052\n",
      "Epoch 2/50\n",
      "480/480 - 39s - loss: 1.3432 - accuracy: 0.4723 - val_loss: 1.8924 - val_accuracy: 0.2740\n",
      "Epoch 3/50\n",
      "480/480 - 39s - loss: 1.1898 - accuracy: 0.5345 - val_loss: 1.8359 - val_accuracy: 0.3153\n",
      "Epoch 4/50\n",
      "480/480 - 39s - loss: 1.1012 - accuracy: 0.5626 - val_loss: 1.9794 - val_accuracy: 0.2857\n",
      "Epoch 5/50\n",
      "480/480 - 38s - loss: 1.0434 - accuracy: 0.5897 - val_loss: 1.8532 - val_accuracy: 0.3630\n",
      "Epoch 6/50\n",
      "480/480 - 39s - loss: 1.0052 - accuracy: 0.6005 - val_loss: 2.0039 - val_accuracy: 0.2781\n",
      "Epoch 7/50\n",
      "480/480 - 38s - loss: 0.9764 - accuracy: 0.6131 - val_loss: 1.9341 - val_accuracy: 0.3390\n",
      "Epoch 8/50\n",
      "480/480 - 39s - loss: 0.9415 - accuracy: 0.6274 - val_loss: 1.9904 - val_accuracy: 0.3230\n",
      "Epoch 9/50\n",
      "480/480 - 39s - loss: 0.9189 - accuracy: 0.6395 - val_loss: 2.0330 - val_accuracy: 0.3385\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Model: \"RNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_7 (SimpleRNN)     (None, 60, 100)           470900    \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 60, 100)           0         \n",
      "_________________________________________________________________\n",
      "layer_normalization_66 (Laye (None, 60, 100)           200       \n",
      "_________________________________________________________________\n",
      "Dense_Output (Dense)         (None, 60, 7)             707       \n",
      "=================================================================\n",
      "Total params: 471,807\n",
      "Trainable params: 471,807\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "480/480 - 22s - loss: 1.4381 - accuracy: 0.5068 - val_loss: 1.8872 - val_accuracy: 0.3143\n",
      "Epoch 2/50\n",
      "480/480 - 21s - loss: 1.0563 - accuracy: 0.6179 - val_loss: 1.8069 - val_accuracy: 0.3724\n",
      "Epoch 3/50\n",
      "480/480 - 21s - loss: 0.9357 - accuracy: 0.6579 - val_loss: 1.8220 - val_accuracy: 0.3942\n",
      "Epoch 4/50\n",
      "480/480 - 21s - loss: 0.8694 - accuracy: 0.6770 - val_loss: 1.8587 - val_accuracy: 0.4372\n",
      "Epoch 5/50\n",
      "480/480 - 22s - loss: 0.8235 - accuracy: 0.6925 - val_loss: 1.9861 - val_accuracy: 0.3991\n",
      "Epoch 6/50\n",
      "480/480 - 21s - loss: 0.8050 - accuracy: 0.6967 - val_loss: 1.8221 - val_accuracy: 0.4271\n",
      "Epoch 7/50\n",
      "480/480 - 22s - loss: 0.7712 - accuracy: 0.7034 - val_loss: 1.9511 - val_accuracy: 0.4048\n",
      "Epoch 8/50\n",
      "480/480 - 21s - loss: 0.7479 - accuracy: 0.7113 - val_loss: 1.8245 - val_accuracy: 0.4284\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Model: \"LSTM\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 60, 100)           1883600   \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 60, 100)           0         \n",
      "_________________________________________________________________\n",
      "layer_normalization_67 (Laye (None, 60, 100)           200       \n",
      "_________________________________________________________________\n",
      "Dense_Output (Dense)         (None, 60, 7)             707       \n",
      "=================================================================\n",
      "Total params: 1,884,507\n",
      "Trainable params: 1,884,507\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "480/480 - 10s - loss: 1.4434 - accuracy: 0.5294 - val_loss: 1.6161 - val_accuracy: 0.4302\n",
      "Epoch 2/50\n",
      "480/480 - 9s - loss: 1.0273 - accuracy: 0.6478 - val_loss: 1.7596 - val_accuracy: 0.4107\n",
      "Epoch 3/50\n",
      "480/480 - 9s - loss: 0.8984 - accuracy: 0.6880 - val_loss: 1.7592 - val_accuracy: 0.4246\n",
      "Epoch 4/50\n",
      "480/480 - 9s - loss: 0.8247 - accuracy: 0.7137 - val_loss: 1.7188 - val_accuracy: 0.4698\n",
      "Epoch 5/50\n",
      "480/480 - 9s - loss: 0.7614 - accuracy: 0.7299 - val_loss: 1.8249 - val_accuracy: 0.4588\n",
      "Epoch 6/50\n",
      "480/480 - 9s - loss: 0.7154 - accuracy: 0.7450 - val_loss: 1.9015 - val_accuracy: 0.4444\n",
      "Epoch 7/50\n",
      "480/480 - 9s - loss: 0.6639 - accuracy: 0.7578 - val_loss: 2.0214 - val_accuracy: 0.4277\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Model: \"FW-RNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "FW-RNN (RNN)                 (32, 60, 100)             781100    \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (32, 60, 100)             0         \n",
      "_________________________________________________________________\n",
      "Dense_Output (Dense)         (32, 60, 7)               707       \n",
      "=================================================================\n",
      "Total params: 781,807\n",
      "Trainable params: 781,807\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "480/480 - 39s - loss: 1.8279 - accuracy: 0.3153 - val_loss: 1.8267 - val_accuracy: 0.2627\n",
      "Epoch 2/50\n",
      "480/480 - 38s - loss: 1.3289 - accuracy: 0.4605 - val_loss: 1.8416 - val_accuracy: 0.3157\n",
      "Epoch 3/50\n",
      "480/480 - 38s - loss: 1.1490 - accuracy: 0.5292 - val_loss: 1.7755 - val_accuracy: 0.3588\n",
      "Epoch 4/50\n",
      "480/480 - 38s - loss: 1.0644 - accuracy: 0.5704 - val_loss: 1.8351 - val_accuracy: 0.3369\n",
      "Epoch 5/50\n",
      "480/480 - 38s - loss: 1.0035 - accuracy: 0.5899 - val_loss: 1.9322 - val_accuracy: 0.3161\n",
      "Epoch 6/50\n",
      "480/480 - 39s - loss: 0.9704 - accuracy: 0.6084 - val_loss: 1.8541 - val_accuracy: 0.3692\n",
      "Epoch 7/50\n",
      "480/480 - 38s - loss: 0.9387 - accuracy: 0.6260 - val_loss: 1.9173 - val_accuracy: 0.3435\n",
      "Epoch 8/50\n",
      "480/480 - 38s - loss: 0.9124 - accuracy: 0.6414 - val_loss: 1.9401 - val_accuracy: 0.3485\n",
      "Epoch 9/50\n",
      "480/480 - 38s - loss: 0.9070 - accuracy: 0.6440 - val_loss: 1.8688 - val_accuracy: 0.3864\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n"
     ]
    }
   ],
   "source": [
    "batchsize = 32\n",
    "\n",
    "# disivible length of train and val features by batchsize\n",
    "train_div = (X_train.shape[0] // batchsize) * batchsize\n",
    "val_div = (X_val.shape[0] // batchsize) * batchsize\n",
    "\n",
    "X_train = X_train[:train_div]\n",
    "y_train = y_train[:train_div]\n",
    "\n",
    "X_val = X_val[:val_div]\n",
    "y_val = y_val[:val_div]\n",
    "\n",
    "\n",
    "for num_units in [5, 20, 50, 100]:\n",
    "    for model in [\"RNN\", \"LSTM\", \"FWRNN\"]:\n",
    "        #     for model in [\"RNN\", \"LSTM\", \"FWRNN\"]:\n",
    "        # Access tensorboard in cmd of the main repo folder with following code:\n",
    "        # tensorboard --logdir='logs/'\n",
    "        name = f\"final_{model}_{num_units}units_rmsprop_dropout0.4\"\n",
    "        tb_callback = tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=f\"logs/models_with_extractedfeatures_vgg19block5/{name}\"\n",
    "        )\n",
    "\n",
    "        if model == \"RNN\":\n",
    "            NN = build_base(model, num_units)\n",
    "        elif model == \"LSTM\":\n",
    "            NN = build_base(model, num_units)\n",
    "        elif model == \"FWRNN\":\n",
    "            NN = build_FWRNN(batchsize, num_units, \"relu\")\n",
    "\n",
    "        NN.summary()\n",
    "        history = NN.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            batch_size=batchsize,\n",
    "            sample_weight=train_samples_weights[:train_div],\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=[es, tb_callback],\n",
    "            epochs=50,\n",
    "            verbose=2,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "        # Plot model\n",
    "        tf.keras.utils.plot_model(\n",
    "            NN,\n",
    "            to_file=f\"data/model_architectures/models_with_extractedfeatures_vgg19block5/{model}_{num_units}units_architecture.png\",\n",
    "            show_shapes=True,\n",
    "            show_dtype=True,\n",
    "            show_layer_names=True,\n",
    "            rankdir=\"LR\",\n",
    "            expand_nested=False,\n",
    "            dpi=96,\n",
    "        )\n",
    "\n",
    "        # Save model\n",
    "        tf.keras.Model.save(\n",
    "            NN,\n",
    "            filepath=f\"data/models/models_with_extractedfeatures_vgg19block5/{model}_{num_units}units.h5\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T14:53:14.852248Z",
     "iopub.status.busy": "2020-11-30T14:53:14.852248Z",
     "iopub.status.idle": "2020-11-30T14:53:14.862246Z",
     "shell.execute_reply": "2020-11-30T14:53:14.862246Z",
     "shell.execute_reply.started": "2020-11-30T14:53:14.852248Z"
    }
   },
   "source": [
    "## Evaluate on Aff-Wild2 Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T20:12:38.809992Z",
     "iopub.status.busy": "2020-11-30T20:12:38.809992Z",
     "iopub.status.idle": "2020-11-30T20:12:38.818991Z",
     "shell.execute_reply": "2020-11-30T20:12:38.818991Z",
     "shell.execute_reply.started": "2020-11-30T20:12:38.809992Z"
    }
   },
   "outputs": [],
   "source": [
    "test_div_AW2 = (X_test.shape[0] // batchsize) * batchsize\n",
    "X_test = X_test[:test_div_AW2]\n",
    "y_test = y_test[:test_div_AW2]\n",
    "\n",
    "test_div_AF7 = (X_test_AF7.shape[0] // batchsize) * batchsize\n",
    "X_test_AF7 =  X_test_AF7[:test_div_AF7]\n",
    "y_test_AF7 = y_test_AF7[:test_div_AF7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T20:31:53.302657Z",
     "iopub.status.busy": "2020-11-30T20:31:53.302657Z",
     "iopub.status.idle": "2020-11-30T20:33:03.801656Z",
     "shell.execute_reply": "2020-11-30T20:33:03.800656Z",
     "shell.execute_reply.started": "2020-11-30T20:31:53.302657Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"RNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_4 (SimpleRNN)     (None, 60, 5)             23070     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 60, 5)             0         \n",
      "_________________________________________________________________\n",
      "layer_normalization_57 (Laye (None, 60, 5)             10        \n",
      "_________________________________________________________________\n",
      "Dense_Output (Dense)         (None, 60, 7)             42        \n",
      "=================================================================\n",
      "Total params: 23,122\n",
      "Trainable params: 23,122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "83/83 [==============================] - 2s 19ms/step - loss: 1.7858 - accuracy: 0.3968\n",
      "{None: [0.6151420395725274, 0.019224924012158053, 0.034700826982325274, 0.020308655781704314, 0.007299855607251724, 0.176051267281106, 0.1908186083220064], 'macro': 0.1519351682227256}\n",
      "34/34 [==============================] - 1s 21ms/step - loss: 2.2376 - accuracy: 0.1745\n",
      "{None: [0.2700325016495198, 0.2043419172735826, 0.05887496356747304, 0.1095348069943913, 0.011858644952170132, 0.1542950120630646, 0.12342114760014436], 'macro': 0.13319414201433513}\n",
      "Model: \"LSTM\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 60, 5)             92280     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 60, 5)             0         \n",
      "_________________________________________________________________\n",
      "layer_normalization_58 (Laye (None, 60, 5)             10        \n",
      "_________________________________________________________________\n",
      "Dense_Output (Dense)         (None, 60, 7)             42        \n",
      "=================================================================\n",
      "Total params: 92,332\n",
      "Trainable params: 92,332\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "83/83 [==============================] - 1s 13ms/step - loss: 1.8375 - accuracy: 0.4083\n",
      "{None: [0.5517490981583444, 0.046686511044963186, 0.015257602675305675, 0.013207924754852912, 0.3657256257601946, 0.1403497513236002, 0.264657934678734], 'macro': 0.19966206405657072}\n",
      "34/34 [==============================] - 1s 14ms/step - loss: 2.3412 - accuracy: 0.1837\n",
      "{None: [0.24355326604570365, 0.09865530555185727, 0.06102098526566453, 0.05670833870344116, 0.30189046337429876, 0.1237697373079374, 0.1279549718574109], 'macro': 0.1447932954437591}\n",
      "Model: \"FW-RNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "FW-RNN (RNN)                 (32, 60, 5)               23855     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (32, 60, 5)               0         \n",
      "_________________________________________________________________\n",
      "Dense_Output (Dense)         (32, 60, 7)               42        \n",
      "=================================================================\n",
      "Total params: 23,897\n",
      "Trainable params: 23,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "83/83 [==============================] - 3s 33ms/step - loss: 1.6340 - accuracy: 0.5076\n",
      "{None: [0.7019036276678198, 0.04731727925644275, 0.005720823798627002, 0.04434456928838951, 0.049926073651273946, 0.21289424860853431, 0.25891853214760296], 'macro': 0.18871787920267005}\n",
      "34/34 [==============================] - 1s 32ms/step - loss: 2.1569 - accuracy: 0.1988\n",
      "{None: [0.30789239265989665, 0.2121542940320233, 0.005271602108640843, 0.03392779469334493, 0.013001083423618633, 0.18363112391930833, 0.04457334758052206], 'macro': 0.11435023405962211}\n",
      "Model: \"RNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_5 (SimpleRNN)     (None, 60, 20)            92580     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 60, 20)            0         \n",
      "_________________________________________________________________\n",
      "layer_normalization_60 (Laye (None, 60, 20)            40        \n",
      "_________________________________________________________________\n",
      "Dense_Output (Dense)         (None, 60, 7)             147       \n",
      "=================================================================\n",
      "Total params: 92,767\n",
      "Trainable params: 92,767\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "83/83 [==============================] - 2s 20ms/step - loss: 1.6798 - accuracy: 0.3663\n",
      "{None: [0.5091854919020913, 0.08117723156532988, 0.024318847106507543, 0.09834619625137818, 0.32803190241613883, 0.1600669269380926, 0.3131124749923706], 'macro': 0.2163198673102727}\n",
      "34/34 [==============================] - 1s 18ms/step - loss: 2.2621 - accuracy: 0.1994\n",
      "{None: [0.2721330275229358, 0.16544985453148156, 0.060014079549454424, 0.06313564918768988, 0.2114775558464592, 0.2472529667958605, 0.11894093686354378], 'macro': 0.16262915289963215}\n",
      "Model: \"LSTM\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 60, 20)            370320    \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 60, 20)            0         \n",
      "_________________________________________________________________\n",
      "layer_normalization_61 (Laye (None, 60, 20)            40        \n",
      "_________________________________________________________________\n",
      "Dense_Output (Dense)         (None, 60, 7)             147       \n",
      "=================================================================\n",
      "Total params: 370,507\n",
      "Trainable params: 370,507\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "83/83 [==============================] - 2s 14ms/step - loss: 1.7520 - accuracy: 0.3479\n",
      "{None: [0.4898005109672636, 0.04879070254423621, 0.019030759017481743, 0.030772518431456355, 0.317659003351831, 0.10695630544730951, 0.337776464941107], 'macro': 0.1929694663858122}\n",
      "34/34 [==============================] - 1s 14ms/step - loss: 2.1497 - accuracy: 0.2126\n",
      "{None: [0.27112496457920093, 0.17082827429657393, 0.03978201634877384, 0.04249168955051308, 0.2852808330172741, 0.2448013367991088, 0.08533944720417783], 'macro': 0.16280693739937463}\n",
      "Model: \"FW-RNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "FW-RNN (RNN)                 (32, 60, 20)              105020    \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (32, 60, 20)              0         \n",
      "_________________________________________________________________\n",
      "Dense_Output (Dense)         (32, 60, 7)               147       \n",
      "=================================================================\n",
      "Total params: 105,167\n",
      "Trainable params: 105,167\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "83/83 [==============================] - 3s 31ms/step - loss: 1.7594 - accuracy: 0.3505\n",
      "{None: [0.4947392344817444, 0.04325099928256636, 0.02176595983395041, 0.02795133179875041, 0.3007567567567567, 0.22984077442637343, 0.26140302232080964], 'macro': 0.19710115412870735}\n",
      "34/34 [==============================] - 1s 33ms/step - loss: 2.2912 - accuracy: 0.2152\n",
      "{None: [0.2921822491989476, 0.2288139727765039, 0.033546813052759986, 0.04556283502084574, 0.21849630808142617, 0.2515605493133583, 0.0850647763383036], 'macro': 0.16503250054030646}\n",
      "Model: \"RNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_6 (SimpleRNN)     (None, 60, 50)            232950    \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 60, 50)            0         \n",
      "_________________________________________________________________\n",
      "layer_normalization_63 (Laye (None, 60, 50)            100       \n",
      "_________________________________________________________________\n",
      "Dense_Output (Dense)         (None, 60, 7)             357       \n",
      "=================================================================\n",
      "Total params: 233,407\n",
      "Trainable params: 233,407\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "83/83 [==============================] - 2s 21ms/step - loss: 1.7787 - accuracy: 0.3685\n",
      "{None: [0.5317043987531094, 0.0950627452880469, 0.002753303964757709, 0.04098132143852802, 0.3321252059308073, 0.14306739748274463, 0.09934084185394716], 'macro': 0.17786217353027728}\n",
      "34/34 [==============================] - 1s 19ms/step - loss: 2.5481 - accuracy: 0.2186\n",
      "{None: [0.2647456630402823, 0.21655542658154386, 0.0015257469802924348, 0.050188569770815206, 0.2973126112999838, 0.23751595065929393, 0.04529558701082431], 'macro': 0.15901993647757653}\n",
      "Model: \"LSTM\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 60, 50)            931800    \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 60, 50)            0         \n",
      "_________________________________________________________________\n",
      "layer_normalization_64 (Laye (None, 60, 50)            100       \n",
      "_________________________________________________________________\n",
      "Dense_Output (Dense)         (None, 60, 7)             357       \n",
      "=================================================================\n",
      "Total params: 932,257\n",
      "Trainable params: 932,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "83/83 [==============================] - 2s 14ms/step - loss: 1.5733 - accuracy: 0.4282\n",
      "{None: [0.5769214477195573, 0.07218398522746348, 0.02410396143366171, 0.01691216584833606, 0.3316458733205374, 0.17208966015907448, 0.3547193935796155], 'macro': 0.2212252124697494}\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 2.4793 - accuracy: 0.2196\n",
      "{None: [0.3040087699628771, 0.0921525283227411, 0.05347497299243789, 0.006518010291595197, 0.3076923076923077, 0.21237747453392275, 0.06615406934571305], 'macro': 0.1489111618773707}\n",
      "Model: \"FW-RNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "FW-RNN (RNN)                 (32, 60, 50)              310550    \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (32, 60, 50)              0         \n",
      "_________________________________________________________________\n",
      "Dense_Output (Dense)         (32, 60, 7)               357       \n",
      "=================================================================\n",
      "Total params: 310,907\n",
      "Trainable params: 310,907\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "83/83 [==============================] - 3s 35ms/step - loss: 1.8438 - accuracy: 0.3174\n",
      "{None: [0.4395506200952663, 0.11139369277721262, 0.020033777881311512, 0.06627291554805553, 0.2837415039378574, 0.21503808725443746, 0.29244342722138134], 'macro': 0.20406771781650318}\n",
      "34/34 [==============================] - 1s 32ms/step - loss: 2.4319 - accuracy: 0.1992\n",
      "{None: [0.23508064516129032, 0.18553053124850574, 0.0579004329004329, 0.13969732246798602, 0.29182291036651753, 0.16841423948220063, 0.043596730245231606], 'macro': 0.1602918302674521}\n",
      "Model: \"RNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_7 (SimpleRNN)     (None, 60, 100)           470900    \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 60, 100)           0         \n",
      "_________________________________________________________________\n",
      "layer_normalization_66 (Laye (None, 60, 100)           200       \n",
      "_________________________________________________________________\n",
      "Dense_Output (Dense)         (None, 60, 7)             707       \n",
      "=================================================================\n",
      "Total params: 471,807\n",
      "Trainable params: 471,807\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "83/83 [==============================] - 2s 21ms/step - loss: 1.7477 - accuracy: 0.3756\n",
      "{None: [0.5316553130893277, 0.04165039698034622, 0.00485319097306479, 0.055687203791469186, 0.30280817309394814, 0.17328765009105782, 0.23644422642302773], 'macro': 0.19234087920603452}\n",
      "34/34 [==============================] - 1s 19ms/step - loss: 2.6917 - accuracy: 0.2144\n",
      "{None: [0.3159292455067103, 0.15536456889381758, 0.009400988309027358, 0.02484271656718826, 0.22818452832095398, 0.24251509473245889, 0.10706282679060503], 'macro': 0.15475713844582306}\n",
      "Model: \"LSTM\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 60, 100)           1883600   \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 60, 100)           0         \n",
      "_________________________________________________________________\n",
      "layer_normalization_67 (Laye (None, 60, 100)           200       \n",
      "_________________________________________________________________\n",
      "Dense_Output (Dense)         (None, 60, 7)             707       \n",
      "=================================================================\n",
      "Total params: 1,884,507\n",
      "Trainable params: 1,884,507\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "83/83 [==============================] - 1s 14ms/step - loss: 1.5622 - accuracy: 0.4392\n",
      "{None: [0.5860350000583724, 0.0763944382291833, 0.003476533399553017, 0.052705223880597014, 0.3261235802402043, 0.12231740441154523, 0.4190150425622886], 'macro': 0.22658103182596342}\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 2.3934 - accuracy: 0.2276\n",
      "{None: [0.31052312173807506, 0.12499221910986616, 0.008637709772951628, 0.02489905787348587, 0.31416486390494336, 0.22043739113605573, 0.05359253907789308], 'macro': 0.15103527180189585}\n",
      "Model: \"FW-RNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "FW-RNN (RNN)                 (32, 60, 100)             781100    \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (32, 60, 100)             0         \n",
      "_________________________________________________________________\n",
      "Dense_Output (Dense)         (32, 60, 7)               707       \n",
      "=================================================================\n",
      "Total params: 781,807\n",
      "Trainable params: 781,807\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "83/83 [==============================] - 3s 35ms/step - loss: 1.7328 - accuracy: 0.3565\n",
      "{None: [0.505277254309125, 0.071910469084848, 0.004922644163150492, 0.07255476489465605, 0.33543073460200534, 0.2487778381314503, 0.12558071585098612], 'macro': 0.19492206014803162}\n",
      "34/34 [==============================] - 1s 32ms/step - loss: 2.5323 - accuracy: 0.2114\n",
      "{None: [0.2584985625796508, 0.09032091097308488, 0.01738383946775405, 0.08641114982578396, 0.31042030310586544, 0.24545683362698215, 0.0870833800920211], 'macro': 0.15651071138159178}\n"
     ]
    }
   ],
   "source": [
    "for num_units in [5, 20, 50, 100]:\n",
    "    for model in [\"RNN\", \"LSTM\", \"FWRNN\"]:\n",
    "        class FW_RNNCell(layers.Layer):\n",
    "            def __init__(\n",
    "                self,\n",
    "                units,\n",
    "                use_bias,\n",
    "                batch_size,\n",
    "                decay_rate,\n",
    "                learning_rate,\n",
    "                activation,\n",
    "                step,\n",
    "                LN=layers.LayerNormalization(),\n",
    "                **kwargs\n",
    "            ):\n",
    "                super(FW_RNNCell, self).__init__(**kwargs)\n",
    "                self.units = units\n",
    "                self.step = step\n",
    "                self.use_bias = use_bias\n",
    "                self.activation = activations.get(activation)\n",
    "                self.l = decay_rate\n",
    "                self.e = learning_rate\n",
    "                self.LN = LN\n",
    "                self.batch = batch_size\n",
    "                self.state_size = self.units\n",
    "\n",
    "                # Initializer & regularizer for the slow input-to-hidden weights matrix\n",
    "                self.C_initializer = initializers.get(\"glorot_uniform\")\n",
    "\n",
    "                # Initializer & regularizer for the slow hidden weights matrix\n",
    "                self.W_h_initializer = initializers.get(\"identity\")\n",
    "\n",
    "                # Initializer & regularizer for the fast weights matrix\n",
    "                self.A_initializer = initializers.get(\"zeros\")\n",
    "\n",
    "                # Initializer for the bias vector.\n",
    "                self.b_x_initializer = initializers.get(\"zeros\")\n",
    "\n",
    "            def build(self, input_shape):\n",
    "                # Build is only called at the start, to initialize all the weights and biases\n",
    "\n",
    "                # C = Slow input-to-hidden weights [shape (4608, 64)]\n",
    "                self.C = self.add_weight(\n",
    "                    shape=(input_shape[-1], self.units),\n",
    "                    name=\"inputweights\",\n",
    "                    initializer=self.C_initializer,\n",
    "                )\n",
    "\n",
    "                # W_h The previous hidden state via the slow transition weights [shape (units, units)]\n",
    "                # they suggest to multiply it with 0.05, so gain = 0.05\n",
    "                self.W_h = self.add_weight(\n",
    "                    shape=(self.units, self.units),\n",
    "                    name=\"hiddenweights\",\n",
    "                    initializer=self.W_h_initializer,\n",
    "                )\n",
    "                self.W_h = tf.scalar_mul(0.05, self.W_h)\n",
    "\n",
    "                # A (fast weights) [shape (batch_size, units, units)]\n",
    "                self.A = self.add_weight(\n",
    "                    shape=(self.batch, self.units, self.units),\n",
    "                    name=\"fastweights\",\n",
    "                    initializer=self.A_initializer,\n",
    "                )\n",
    "\n",
    "                if self.use_bias:\n",
    "                    self.bias = self.add_weight(\n",
    "                        shape=(self.units,), name=\"bias\", initializer=self.b_x_initializer,\n",
    "                    )\n",
    "                else:\n",
    "                    self.bias = None\n",
    "                self.built = True\n",
    "\n",
    "            def call(self, inputs, states, training=None):\n",
    "                prev_output = states[0] if nest.is_sequence(states) else states\n",
    "\n",
    "                # Next hidden state h(t+1) is computed in two steps:\n",
    "                # Step 1 calculate preliminary vector: h_0(t+1) = f(W_h ⋅ h(t) + C ⋅ x(t))\n",
    "                h = K.dot(prev_output, self.W_h) + K.dot(inputs, self.C)\n",
    "                if self.bias is not None:\n",
    "                    h = h + self.bias\n",
    "                if self.activation is not None:\n",
    "                    h = self.activation(h)\n",
    "\n",
    "                # Reshape h to use with a\n",
    "                h_s = tf.reshape(h, [self.batch, 1, self.units])\n",
    "\n",
    "                # Define preliminary vector in variable\n",
    "                prelim = tf.reshape(K.dot(prev_output, self.W_h), (h_s.shape)) + tf.reshape(\n",
    "                    K.dot(inputs, self.C), (h_s.shape)\n",
    "                )\n",
    "\n",
    "                # Fast weights update rule: A(t) = λ*A(t-1) + η*h(t) ⋅ h(t)^T\n",
    "                self.A.assign(\n",
    "                    tf.math.add(\n",
    "                        tf.scalar_mul(self.l, self.A),\n",
    "                        tf.scalar_mul(\n",
    "                            self.e, tf.linalg.matmul(tf.transpose(h_s, [0, 2, 1]), h_s)\n",
    "                        ),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # Step 2: Initiate inner loop with preliminary vector, which runs for S steps\n",
    "                # to progressively change the hidden state into h(t+1) = h_s(t+1)\n",
    "                # h_s+1(t+1) f([W_h ⋅ h(t) + C ⋅ x(t)]) + A(t)h_s(t+1)\n",
    "                for _ in range(self.step):\n",
    "                    h_s = tf.math.add(prelim, tf.linalg.matmul(h_s, self.A))\n",
    "                    if self.activation is not None:\n",
    "                        h_s = self.activation(h_s)\n",
    "\n",
    "                    # Apply layer normalization on hidden state\n",
    "                    h_s = self.LN(h_s)\n",
    "\n",
    "                h = tf.reshape(h_s, [self.batch, self.units])\n",
    "\n",
    "                output = h\n",
    "                new_state = [output] if nest.is_sequence(states) else output\n",
    "                return output, new_state\n",
    "\n",
    "            def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n",
    "                return _generate_zero_filled_state_for_cell(self, inputs, batch_size, dtype)\n",
    "\n",
    "            def get_config(self):\n",
    "                config = {\n",
    "                    \"units\": self.units,\n",
    "                    \"step\": self.step,\n",
    "                    \"batch_size\": self.batch,\n",
    "                    \"use_bias\": self.use_bias,\n",
    "                    \"activation\": activations.serialize(self.activation),\n",
    "                    \"decay_rate\": self.l,\n",
    "                    \"learning_rate\": self.e,\n",
    "                    \"LN\": self.LN,\n",
    "                }\n",
    "                base_config = super(FW_RNNCell, self).get_config()\n",
    "                return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "        if model == \"FWRNN\":\n",
    "            NN = tf.keras.models.load_model(\n",
    "                filepath=f\"data/models/models_with_extractedfeatures_vgg19block5/{model}_{num_units}units.h5\",\n",
    "                custom_objects={\"FW_RNNCell\": FW_RNNCell},\n",
    "                compile=True,\n",
    "            )\n",
    "        else:\n",
    "            NN = tf.keras.models.load_model(\n",
    "                filepath=f\"data/models/models_with_extractedfeatures_vgg19block5/{model}_{num_units}units.h5\",\n",
    "                compile=True,\n",
    "            )\n",
    "\n",
    "        NN.summary()\n",
    "        \n",
    "        #####################################################################################    \n",
    "#         # Evaluate on validation set to get validation scores\n",
    "#         csvlog_AW2_val = tf.keras.callbacks.CSVLogger(\n",
    "#             f\"data/models/val_scores/{model}_{num_units}units_AW2_validation_scores.csv\",\n",
    "#             separator=\",\",\n",
    "#             append=False,\n",
    "#         )\n",
    "#         NN.evaluate(\n",
    "#             X_val, y_val, batch_size=batchsize, callbacks=[csvlog_AW2_val],\n",
    "#         )\n",
    "\n",
    "#         # Get F1 scores for validation set\n",
    "#         val_pred = NN.predict(X_val, verbose=0)\n",
    "#         val_pred = np.reshape(\n",
    "#             val_pred, (val_pred.shape[0] * val_pred.shape[1], val_pred.shape[2])\n",
    "#         )\n",
    "#         # Convert one hot encoding to integers\n",
    "#         val_pred = np.argmax(val_pred, axis=1)\n",
    "\n",
    "#         # Reshape back to (frame, label)\n",
    "#         val_true = np.reshape(\n",
    "#             y_val, (y_val.shape[0] * y_val.shape[1], y_val.shape[2],),\n",
    "#         )\n",
    "#         val_true = np.argmax(val_true, axis=1)\n",
    "#         f1scores_val = {\n",
    "#             avg: f1_score(val_true, val_pred, average=avg) for avg in [None, \"macro\"]\n",
    "#         }\n",
    "#         f1scores_val[None] = f1scores_val.get(None).tolist()\n",
    "#         print(f1scores_val)\n",
    "\n",
    "#         with open(\n",
    "#             f\"data/models/val_scores/{model}_{num_units}units_AW2_validation_F1scores.json\",\n",
    "#             \"w\",\n",
    "#         ) as fp:\n",
    "#             json.dump(f1scores_val, fp)\n",
    "            \n",
    "        #####################################################################################    \n",
    "        # Evaluate on test set of AW2 to get test scores\n",
    "        csvlog_AW2_test = tf.keras.callbacks.CSVLogger(\n",
    "            f\"data/models/test_scores/{model}_{num_units}units_AW2_test_scores.csv\",\n",
    "            separator=\",\",\n",
    "            append=False,\n",
    "        )\n",
    "        NN.evaluate(\n",
    "            X_test, y_test, batch_size=batchsize, callbacks=[csvlog_AW2_test],\n",
    "        )\n",
    "\n",
    "        # Get F1 scores for AF7 test set\n",
    "        test_pred = NN.predict(X_test, verbose=0)\n",
    "        test_pred = np.reshape(\n",
    "            test_pred, (test_pred.shape[0] * test_pred.shape[1], test_pred.shape[2])\n",
    "        )\n",
    "        # Convert one hot encoding to integers\n",
    "        test_pred = np.argmax(test_pred, axis=1)\n",
    "\n",
    "        # Reshape back to (frame, label)\n",
    "        test_true = np.reshape(\n",
    "            y_test, (y_test.shape[0] * y_test.shape[1], y_test.shape[2],),\n",
    "        )\n",
    "        test_true = np.argmax(test_true, axis=1)\n",
    "\n",
    "        f1scores_test = {\n",
    "            avg: f1_score(test_pred, test_true, average=avg) for avg in [None, \"macro\"]\n",
    "        }\n",
    "        f1scores_test[None] = f1scores_test.get(None).tolist()\n",
    "        print(f1scores_test)\n",
    "\n",
    "        with open(\n",
    "            f\"data/models/test_scores/{model}_{num_units}units_AW2_test_F1scores.json\",\n",
    "            \"w\",\n",
    "        ) as fp:\n",
    "            json.dump(f1scores_test, fp)\n",
    "        \n",
    "        ##################################################################################\n",
    "        \n",
    "        # Evaluate on test set of AF7 to get test scores for cross-dataset performance\n",
    "        csvlog_AF7 = tf.keras.callbacks.CSVLogger(\n",
    "            f\"data/models/test_scores/{model}_{num_units}units_AF7_test_scores.csv\",\n",
    "            separator=\",\",\n",
    "            append=False,\n",
    "        )\n",
    "        NN.evaluate(\n",
    "            X_test_AF7, y_test_AF7, batch_size=batchsize, callbacks=[csvlog_AF7],\n",
    "        )\n",
    "\n",
    "        # Get F1 scores for AF7 test set\n",
    "        test_pred = NN.predict(X_test_AF7, verbose=0)\n",
    "        test_pred = np.reshape(\n",
    "            test_pred, (test_pred.shape[0] * test_pred.shape[1], test_pred.shape[2])\n",
    "        )\n",
    "        # Convert one hot encoding to integers\n",
    "        test_pred = np.argmax(test_pred, axis=1)\n",
    "\n",
    "        # Reshape back to (frame, label)\n",
    "        test_true = np.reshape(\n",
    "            y_test_AF7,\n",
    "            (y_test_AF7.shape[0] * y_test_AF7.shape[1], y_test_AF7.shape[2],),\n",
    "        )\n",
    "        test_true = np.argmax(test_true, axis=1)\n",
    "\n",
    "        f1scores_test = {\n",
    "            avg: f1_score(test_pred, test_true, average=avg) for avg in [None, \"macro\"]\n",
    "        }\n",
    "        f1scores_test[None] = f1scores_test.get(None).tolist()\n",
    "        print(f1scores_test)\n",
    "\n",
    "        with open(\n",
    "            f\"data/models/test_scores/{model}_{num_units}units_AF7_test_F1scores.json\",\n",
    "            \"w\",\n",
    "        ) as fp:\n",
    "            json.dump(f1scores_test, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "output_auto_scroll": true,
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
