{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load/import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import scipy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.backend as K\n",
    "import kerastuner as kt\n",
    "\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from os import listdir\n",
    "from os.path import join, splitext, normpath\n",
    "\n",
    "# Import packages to run custom FW-RNN cell\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.keras import activations, initializers\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n",
    "# Import variables and functions from my own scripts\n",
    "from functions import f1, plot_history, arr_replacevalue\n",
    "from load_features import (\n",
    "    train_features_AW2,\n",
    "    val_features_AW2,\n",
    "    train_labels_AW2,\n",
    "    val_labels_AW2,\n",
    "    labels_reshaper,\n",
    "    features_reshaper,\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Limit GPU memory usage\n",
    "for device in tf.config.experimental.list_physical_devices(\"GPU\"):\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data to specified sequence length\n",
    "length = 60\n",
    "seq_train_features = features_reshaper(train_features_AW2, length)\n",
    "seq_val_features = features_reshaper(val_features_AW2, length)\n",
    "\n",
    "seq_train_labels = labels_reshaper(train_labels_AW2, length)\n",
    "seq_val_labels = labels_reshaper(val_labels_AW2, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_sampleweights(labels):\n",
    "    # Convert one-hot encoded labels back to label integers\n",
    "    train_label_ints = np.argmax(labels, axis=2)\n",
    "\n",
    "    # Compute class weights with sklearn\n",
    "    class_weights = class_weight.compute_class_weight(\n",
    "        \"balanced\", np.unique(train_label_ints), train_label_ints.flatten()\n",
    "    )\n",
    "    d_class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "    # Copy label integer array\n",
    "    arr = train_label_ints.copy()\n",
    "\n",
    "    # Pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample\n",
    "    return arr_replacevalue(arr, d_class_weights)\n",
    "    \n",
    "train_samples_weights = comp_sampleweights(seq_train_labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T12:32:51.580486Z",
     "iopub.status.busy": "2020-11-11T12:32:51.580486Z",
     "iopub.status.idle": "2020-11-11T12:32:51.598489Z",
     "shell.execute_reply": "2020-11-11T12:32:51.597488Z",
     "shell.execute_reply.started": "2020-11-11T12:32:51.580486Z"
    }
   },
   "source": [
    "## Possibilities for custom FW-RNN building\n",
    "-  Build a class fw_layer first to define the inner computation block\n",
    "-  Build the FW-RNN model class to define the outer model (which will be trained)\n",
    "\n",
    "# Best option:\n",
    "-  Build custom FW_RNN cell and wrap it in RNN(FW_RNN)\n",
    "    -  The cell abstraction, together with the generic keras.layers.RNN class, make it very easy to implement custom RNN architectures for your research.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T15:27:37.993463Z",
     "iopub.status.busy": "2020-11-11T15:27:37.993463Z",
     "iopub.status.idle": "2020-11-11T15:27:38.000462Z",
     "shell.execute_reply": "2020-11-11T15:27:38.000462Z",
     "shell.execute_reply.started": "2020-11-11T15:27:37.993463Z"
    }
   },
   "source": [
    "# Create FW-RNN Cell\n",
    "Created by using this guide: https://www.tensorflow.org/guide/keras/custom_layers_and_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/layers/recurrent.py#L1218-L1416\n",
    "        \n",
    "def _generate_zero_filled_state_for_cell(cell, inputs, batch_size, dtype):\n",
    "    if inputs is not None:\n",
    "        batch_size = array_ops.shape(inputs)[0]\n",
    "        dtype = inputs.dtype\n",
    "    return _generate_zero_filled_state(batch_size, cell.state_size, dtype)\n",
    "\n",
    "def _generate_zero_filled_state(batch_size_tensor, state_size, dtype):\n",
    "    \"\"\"Generate a zero filled tensor with shape [batch_size, state_size].\"\"\"\n",
    "    if batch_size_tensor is None or dtype is None:\n",
    "        raise ValueError(\n",
    "            'batch_size and dtype cannot be None while constructing initial state: '\n",
    "            'batch_size={}, dtype={}'.format(batch_size_tensor, dtype))\n",
    "\n",
    "    def create_zeros(unnested_state_size):\n",
    "        flat_dims = tensor_shape.TensorShape(unnested_state_size).as_list()\n",
    "        init_state_size = [batch_size_tensor] + flat_dims\n",
    "        return array_ops.zeros(init_state_size, dtype=dtype)\n",
    "\n",
    "    if nest.is_nested(state_size):\n",
    "        return nest.map_structure(create_zeros, state_size)\n",
    "    else:\n",
    "        return create_zeros(state_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FW_RNNCell(layers.Layer):\n",
    "    def __init__(self, units, use_bias, activation, step, **kwargs):\n",
    "        super(FW_RNNCell, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.step = step\n",
    "        self.use_bias = use_bias\n",
    "        self.activation = activations.get(activation)\n",
    "\n",
    "        self.state_size = self.units\n",
    "        self.output_size = self.units\n",
    "\n",
    "        # Initializer for the kernel weights matrix, used for the linear transformation of the inputs\n",
    "        self.kernel_initializer = initializers.get(\"glorot_uniform\")\n",
    "\n",
    "        # Initializer for the bias vector.\n",
    "        self.bias_initializer = initializers.get(\"zeros\")\n",
    "\n",
    "        # Initializer for the recurrent_kernel (hidden) weights matrix, used for the linear\n",
    "        # transformation of the recurrent state.\n",
    "        self.recurrent_initializer = initializers.get(\"identity\")\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            name=\"kernel\",\n",
    "            initializer=self.kernel_initializer,\n",
    "        )\n",
    "        self.recurrent_kernel = self.add_weight(\n",
    "            shape=(self.units, self.units),\n",
    "            name=\"recurrent_kernel\",\n",
    "            initializer=self.recurrent_initializer,\n",
    "        )\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(\n",
    "                shape=(self.units,), name=\"bias\", initializer=self.bias_initializer,\n",
    "            )\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, states, training=None):\n",
    "        prev_output = states[0] if nest.is_sequence(states) else states\n",
    "\n",
    "        h = K.dot(inputs, self.kernel)\n",
    "        if self.bias is not None:\n",
    "            h = K.bias_add(h, self.bias)\n",
    "\n",
    "        output = h + K.dot(prev_output, self.recurrent_kernel)\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "\n",
    "        new_state = [output] if nest.is_sequence(states) else output\n",
    "        return output, new_state\n",
    "\n",
    "    def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n",
    "        return _generate_zero_filled_state_for_cell(self, inputs, batch_size, dtype)\n",
    "\n",
    "#     def get_config(self):\n",
    "#         config = {\n",
    "#             \"units\": self.units,\n",
    "#             \"activation\": activations.serialize(self.activation),\n",
    "#             \"use_bias\": self.use_bias,\n",
    "#             \"kernel_initializer\": initializers.serialize(self.kernel_initializer),\n",
    "#             \"recurrent_initializer\": initializers.serialize(self.recurrent_initializer),\n",
    "#         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential(name=\"FW-RNN\")\n",
    "    model.add(layers.InputLayer(input_shape= (seq_train_features.shape[1], seq_train_features.shape[2])))\n",
    "    model.add(\n",
    "        layers.RNN(\n",
    "            FW_RNNCell(units=512, use_bias=True, activation=\"tanh\", step=1),\n",
    "            return_sequences=True,\n",
    "            name = 'FW-RNN'\n",
    "        )\n",
    "    )\n",
    "    model.add(layers.LayerNormalization())\n",
    "    model.add(layers.Dense(7, activation=\"softmax\", name=\"Dense_Output\"))\n",
    "    model.compile(\n",
    "        optimizer=\"adagrad\",\n",
    "        loss=CategoricalCrossentropy(label_smoothing=0.1),\n",
    "        metrics=[\"accuracy\", f1, \"AUC\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fw_rnn = build_model()\n",
    "fw_rnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train + Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# AW2_norm_minitrain = AW2_norm_minitrain.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "history_best = fw_rnn.fit(\n",
    "    seq_train_features,\n",
    "    seq_train_labels,\n",
    "    sample_weight=train_samples_weights,\n",
    "    validation_data=(seq_val_features, seq_val_labels),\n",
    "    epochs=50,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class fw_layer(layers.Layer):\n",
    "#     def __init__(\n",
    "#         self, units=32, input_shape=(params.sequence_dim, params.features_dim)\n",
    "#     ):\n",
    "#         # A layer encapsulates both a state (the layer's \"weights\")\n",
    "#         super(fw_layers, self)\n",
    "#         # Define weight initializers of input weights and input biases\n",
    "#         W_init = tf.keras.initializers.GlorotUniform()\n",
    "#         b_init = tf.keras.initializers.zeros()\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         #         a transformation from inputs to outputs (a \"call\", the layer's forward pass)\n",
    "#         return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class fw_model(tf.keras.Model):\n",
    "#     def __init__(self, **kwargs):\n",
    "#         super(CustomModel, self).__init__(**kwargs)\n",
    "\n",
    "        \n",
    "#     def call(self, inputs):\n",
    "#         x = self."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ops.executing_eagerly_outside_functions()\n",
    "# from tensorflow.python.distribute import distribution_strategy_context as ds_context\n",
    "# from tensorflow.python.eager import context\n",
    "# from tensorflow.python.keras.engine.base_layer import Layer\n",
    "# from tensorflow.python.keras.engine.input_spec import InputSpec\n",
    "# from tensorflow.python.keras.saving.saved_model import layer_serialization\n",
    "# from tensorflow.python.keras.utils import control_flow_util\n",
    "# from tensorflow.python.keras.utils import generic_utils\n",
    "# from tensorflow.python.keras.utils import tf_utils\n",
    "# from tensorflow.python.ops import control_flow_ops\n",
    "# from tensorflow.python.ops import math_ops\n",
    "# from tensorflow.python.ops import state_ops\n",
    "# from tensorflow.python.platform import tf_logging as logging\n",
    "# from tensorflow.python.training.tracking import base as trackable\n",
    "# from tensorflow.python.training.tracking import data_structures\n",
    "# from tensorflow.python.util.tf_export import keras_export\n",
    "# from tensorflow.tools.docs import doc_controls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "output_auto_scroll": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
